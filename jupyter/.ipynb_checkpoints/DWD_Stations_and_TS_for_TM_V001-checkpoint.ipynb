{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DWD CDC Time Series, Merge with Station Description and Append \n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) from the DWD Climate Data Center in such a way that it can be used with the **QGIS time manager extension**. \n",
    "\n",
    "This extension allows to filter an attribute table of a vector layer (e.g. points representing precipitation stations plus precipitation data) with a time stamp column. The extension limits the attribute table to the records matching the particular time stamp provided by the time manager extension (e.g. by the user moving the time slider). This selected subset of the attribute table is then used to change the sympology of the vector layer according to the variable of interest (e.g. precipitation rate).\n",
    "\n",
    "The QGIS time manager extension approach is a bit brute force, because each individual measurement at a station at a given time is one feature (row in the table), i.e. a time series at station X with hourly resolution for a day (24 values) entails 24 different features with the same station id and the corresponding coordinates but different times. As of now this 1:n relationship can only be realized by importing a CSV file with the according structure. \n",
    "\n",
    "(At least I was not able to generate the required view on a 1:n relationship by merging a point vector layer with precipitation station locations and an imported CSV time series table.)\n",
    "\n",
    "The final data format is a concatenation of time series together with geographic location in 2D (e.g. lat, lon). The required data format looks principly like this:\n",
    "\n",
    "\n",
    "| station_id |        name        |   lat   |   lon  |        meas_time       | prec_rate |\n",
    "|:----------:|:------------------:|:-------:|:------:|:----------------------:|:---------:|\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T08:00:00UTC |       1.5 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T09:00:00UTC |       1.7 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T10:00:00UTC |       0.1 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T08:00:00UTC |       0.8 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T09:00:00UTC |       0.4 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T10:00:00UTC |       0.0 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "\n",
    "\n",
    "(Table generated with https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "To achieve this the precipitation time series (station_id, meas_time, prec_rate) have to be merged with the station metadata (station_id, lat, lon) coming from the a CSV file generated in an earlier activity. We use Pandas to read, join and append the data to generate the final CSV file to be imported as a point layer to QGIS. \n",
    "\n",
    "This final data format is far from being optimal because of large size and highly redundant information. This is a challenge for QGIS which loses responsiveness with large data. To jsut show the principle it is advisable to limit to size of the problem. \n",
    "\n",
    "The following filters (selection criteria) are applied:\n",
    "\n",
    "  * Precipitation stations in NRW only (approx. 127 stations) \n",
    "  * Hourly precipitation data\n",
    "  * Time interval from 2018-12-01 to last date in precipitation data set \n",
    "  \n",
    "Still: 40 days * 24 hrs / day * 127 stations = 121920 records leading to 121920 features in a point layer in QGIS. \n",
    "\n",
    "In fact, the resulting number of records is arround 91000. The reason might be that not all stations in the station list have time series. This has to be checked carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTP Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "topic_dir = \"/hourly/precipitation/historical/\"\n",
    "#topic_dir = \"/hourly/precipitation/recent/\"\n",
    "#topic_dir = \"/annual/kl/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"../data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"../data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallelly merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/original/DWD/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "\n",
      "../data/generated/DWD/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Grab File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabFile(ftpfullname,localfullname):\n",
    "    try:\n",
    "        ret = ftp.cwd(\".\") # A dummy action to chack the connection and to provoke an exception if necessary.\n",
    "        localfile = open(localfullname, 'wb')\n",
    "        ftp.retrbinary('RETR ' + ftpfullname, localfile.write, 1024)\n",
    "        localfile.close()\n",
    "    \n",
    "    except ftplib.error_perm:\n",
    "        print(\"FTP ERROR. Operation not permitted. File not found?\")\n",
    "\n",
    "    except ftplib.error_temp:\n",
    "        print(\"FTP ERROR. Timeout.\")\n",
    "\n",
    "    except ConnectionAbortedError:\n",
    "        print(\"FTP ERROR. Connection aborted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def gen_df_from_ftp_dir_listing(ftp, ftpdir):\n",
    "    lines = []\n",
    "    flist = []\n",
    "    try:    \n",
    "        res = ftp.retrlines(\"LIST \"+ftpdir, lines.append)\n",
    "    except:\n",
    "        print(\"Error: ftp.retrlines() failed. ftp timeout? Reconnect!\")\n",
    "        return\n",
    "        \n",
    "    if len(lines) == 0:\n",
    "        print(\"Error: ftp dir is empty\")\n",
    "        return\n",
    "    \n",
    "    for line in lines:\n",
    "#        print(line)\n",
    "        [ftype, fsize, fname] = [line[0:1], int(line[31:42]), line[56:]]\n",
    "#        itemlist = [line[0:1], int(line[31:42]), line[56:]]\n",
    "#        flist.append(itemlist)\n",
    "        \n",
    "        fext = os.path.splitext(fname)[-1]\n",
    "        \n",
    "        if fext == \".zip\":\n",
    "            station_id = int(fname.split(\"_\")[2])\n",
    "        else:\n",
    "            station_id = -1 \n",
    "        \n",
    "        flist.append([station_id, fname, fext, fsize, ftype])\n",
    "        \n",
    "        \n",
    "\n",
    "    df_ftpdir = pd.DataFrame(flist,columns=[\"station_id\", \"name\", \"ext\", \"size\", \"type\"])\n",
    "    return(df_ftpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>BESCHREIBUNG_obsgermany_climate_hourly_precipi...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>71445</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>DESCRIPTION_obsgermany_climate_hourly_precipit...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>69716</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>RR_Stundenwerte_Beschreibung_Stationen.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>209079</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>419265</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>stundenwerte_RR_00020_20040814_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>407378</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44</td>\n",
       "      <td>stundenwerte_RR_00044_20070401_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>320516</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53</td>\n",
       "      <td>stundenwerte_RR_00053_20051001_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>361931</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>stundenwerte_RR_00071_20041022_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>402880</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73</td>\n",
       "      <td>stundenwerte_RR_00073_20070401_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>333070</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78</td>\n",
       "      <td>stundenwerte_RR_00078_20041101_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>384729</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                                               name   ext  \\\n",
       "0          -1  BESCHREIBUNG_obsgermany_climate_hourly_precipi...  .pdf   \n",
       "1          -1  DESCRIPTION_obsgermany_climate_hourly_precipit...  .pdf   \n",
       "2          -1         RR_Stundenwerte_Beschreibung_Stationen.txt  .txt   \n",
       "3           3   stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip   \n",
       "4          20   stundenwerte_RR_00020_20040814_20191231_hist.zip  .zip   \n",
       "5          44   stundenwerte_RR_00044_20070401_20191231_hist.zip  .zip   \n",
       "6          53   stundenwerte_RR_00053_20051001_20191231_hist.zip  .zip   \n",
       "7          71   stundenwerte_RR_00071_20041022_20191231_hist.zip  .zip   \n",
       "8          73   stundenwerte_RR_00073_20070401_20191231_hist.zip  .zip   \n",
       "9          78   stundenwerte_RR_00078_20041101_20191231_hist.zip  .zip   \n",
       "\n",
       "     size type  \n",
       "0   71445    -  \n",
       "1   69716    -  \n",
       "2  209079    -  \n",
       "3  419265    -  \n",
       "4  407378    -  \n",
       "5  320516    -  \n",
       "6  361931    -  \n",
       "7  402880    -  \n",
       "8  333070    -  \n",
       "9  384729    -  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ftpdir.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>419265</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stundenwerte_RR_00020_20040814_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>407378</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>stundenwerte_RR_00044_20070401_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>320516</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stundenwerte_RR_00053_20051001_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>361931</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>stundenwerte_RR_00071_20041022_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>402880</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>stundenwerte_RR_00073_20070401_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>333070</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>stundenwerte_RR_00078_20041101_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>384729</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>stundenwerte_RR_00087_20050201_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>379869</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>stundenwerte_RR_00091_20040901_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>394972</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>stundenwerte_RR_00103_20040701_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>402997</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        name   ext    size  \\\n",
       "station_id                                                                   \n",
       "3           stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip  419265   \n",
       "20          stundenwerte_RR_00020_20040814_20191231_hist.zip  .zip  407378   \n",
       "44          stundenwerte_RR_00044_20070401_20191231_hist.zip  .zip  320516   \n",
       "53          stundenwerte_RR_00053_20051001_20191231_hist.zip  .zip  361931   \n",
       "71          stundenwerte_RR_00071_20041022_20191231_hist.zip  .zip  402880   \n",
       "73          stundenwerte_RR_00073_20070401_20191231_hist.zip  .zip  333070   \n",
       "78          stundenwerte_RR_00078_20041101_20191231_hist.zip  .zip  384729   \n",
       "87          stundenwerte_RR_00087_20050201_20191231_hist.zip  .zip  379869   \n",
       "91          stundenwerte_RR_00091_20040901_20191231_hist.zip  .zip  394972   \n",
       "103         stundenwerte_RR_00103_20040701_20191231_hist.zip  .zip  402997   \n",
       "\n",
       "           type  \n",
       "station_id       \n",
       "3             -  \n",
       "20            -  \n",
       "44            -  \n",
       "53            -  \n",
       "71            -  \n",
       "73            -  \n",
       "78            -  \n",
       "87            -  \n",
       "91            -  \n",
       "103           -  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(station_fname)\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabFile: \n",
      "From: /climate_environment/CDC/observations_germany/climate//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n",
      "To:   ../data/original/DWD//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"grabFile: \")\n",
    "print(\"From: \" + ftp_dir + station_fname)\n",
    "print(\"To:   \" + local_ftp_station_dir + station_fname)\n",
    "grabFile(ftp_dir + station_fname, local_ftp_station_dir + station_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract column names. They are in German (de)\n",
    "# We have to use codecs because of difficulties with character encoding (German Umlaute)\n",
    "import codecs\n",
    "\n",
    "def station_desc_txt_to_csv(txtfile, csvfile):\n",
    "    file = codecs.open(txtfile,\"r\",\"utf-8\")\n",
    "    r = file.readline()\n",
    "    file.close()\n",
    "    colnames_de = r.split()\n",
    "    colnames_de\n",
    "    \n",
    "    translate = \\\n",
    "    {'Stations_id':'station_id',\n",
    "     'von_datum':'date_from',\n",
    "     'bis_datum':'date_to',\n",
    "     'Stationshoehe':'altitude',\n",
    "     'geoBreite': 'latitude',\n",
    "     'geoLaenge': 'longitude',\n",
    "     'Stationsname':'name',\n",
    "     'Bundesland':'state'}\n",
    "    \n",
    "    colnames_en = [translate[h] for h in colnames_de]\n",
    "    \n",
    "    # Skip the first two rows and set the column names.\n",
    "    df = pd.read_fwf(txtfile,skiprows=2,names=colnames_en, parse_dates=[\"date_from\",\"date_to\"],index_col = 0)\n",
    "    \n",
    "    # write csv\n",
    "    df.to_csv(csvfile, sep = \";\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>202</td>\n",
       "      <td>50.7827</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2004-08-14</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>432</td>\n",
       "      <td>48.9220</td>\n",
       "      <td>9.9129</td>\n",
       "      <td>Abtsgmünd-Untergröningen</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>44</td>\n",
       "      <td>52.9336</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2005-10-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>60</td>\n",
       "      <td>52.5850</td>\n",
       "      <td>13.5634</td>\n",
       "      <td>Ahrensfelde</td>\n",
       "      <td>Brandenburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2004-10-22</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>759</td>\n",
       "      <td>48.2156</td>\n",
       "      <td>8.9784</td>\n",
       "      <td>Albstadt-Badkap</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "3          1995-09-01 2011-04-01       202   50.7827     6.0941   \n",
       "20         2004-08-14 2021-01-25       432   48.9220     9.9129   \n",
       "44         2007-04-01 2021-01-25        44   52.9336     8.2370   \n",
       "53         2005-10-01 2021-01-25        60   52.5850    13.5634   \n",
       "71         2004-10-22 2020-01-01       759   48.2156     8.9784   \n",
       "\n",
       "                                name                state  \n",
       "station_id                                                 \n",
       "3                             Aachen  Nordrhein-Westfalen  \n",
       "20          Abtsgmünd-Untergröningen    Baden-Württemberg  \n",
       "44                      Großenkneten        Niedersachsen  \n",
       "53                       Ahrensfelde          Brandenburg  \n",
       "71                   Albstadt-Badkap    Baden-Württemberg  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Stations Located in NRW from Station Description Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    3,   216,   326,   389,   390,   554,   555,   599,   603,\n",
       "              613,   617,   644,   796,   871,   902,   934,   989,  1024,\n",
       "             1046,  1078,  1241,  1246,  1300,  1303,  1327,  1590,  1595,\n",
       "             1766,  2027,  2110,  2254,  2473,  2483,  2497,  2629,  2667,\n",
       "             2703,  2810,  2947,  2968,  2999,  3028,  3031,  3081,  3098,\n",
       "             3215,  3321,  3339,  3499,  3540,  3591,  3795,  3913,  4063,\n",
       "             4127,  4150,  4154,  4313,  4368,  4371,  4400,  4488,  4692,\n",
       "             4741,  4849,  5064,  5347,  5360,  5468,  5480,  5513,  5619,\n",
       "             5699,  5717,  5719,  5733,  6197,  6264,  6276,  6313,  6337,\n",
       "             7106,  7330,  7344,  7374,  7378, 13669, 13670, 13671, 13696,\n",
       "            13700, 13713, 15000],\n",
       "           dtype='int64', name='station_id')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_ids_selected = df_stations[df_stations['state'].str.contains(\"Nordrhein\")].index\n",
    "station_ids_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>101</td>\n",
       "      <td>51.4789</td>\n",
       "      <td>7.2697</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13671</th>\n",
       "      <td>2007-12-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>221</td>\n",
       "      <td>50.9655</td>\n",
       "      <td>7.2753</td>\n",
       "      <td>Overath-Böke</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13696</th>\n",
       "      <td>2007-12-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>60</td>\n",
       "      <td>51.5966</td>\n",
       "      <td>7.4048</td>\n",
       "      <td>Waltrop-Abdinghof</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13700</th>\n",
       "      <td>2008-05-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>205</td>\n",
       "      <td>51.3329</td>\n",
       "      <td>7.3411</td>\n",
       "      <td>Gevelsberg-Oberbröking</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13713</th>\n",
       "      <td>2007-11-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>386</td>\n",
       "      <td>51.0899</td>\n",
       "      <td>7.6289</td>\n",
       "      <td>Meinerzhagen-Redlendorf</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>231</td>\n",
       "      <td>50.7983</td>\n",
       "      <td>6.0244</td>\n",
       "      <td>Aachen-Orsbach</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2021-01-25       298   51.1143     7.8807   \n",
       "389        2009-11-01 2021-01-25       436   51.0148     8.4318   \n",
       "390        2004-07-01 2021-01-25       610   50.9837     8.3683   \n",
       "554        1995-09-01 2021-01-25        23   51.8293     6.5365   \n",
       "555        2008-01-01 2018-11-01       101   51.4789     7.2697   \n",
       "...               ...        ...       ...       ...        ...   \n",
       "13671      2007-12-01 2021-01-25       221   50.9655     7.2753   \n",
       "13696      2007-12-01 2021-01-25        60   51.5966     7.4048   \n",
       "13700      2008-05-01 2021-01-25       205   51.3329     7.3411   \n",
       "13713      2007-11-01 2021-01-25       386   51.0899     7.6289   \n",
       "15000      2011-04-01 2021-01-25       231   50.7983     6.0244   \n",
       "\n",
       "                                    name                state  \n",
       "station_id                                                     \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen  \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen  \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen  \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen  \n",
       "555                               Bochum  Nordrhein-Westfalen  \n",
       "...                                  ...                  ...  \n",
       "13671                       Overath-Böke  Nordrhein-Westfalen  \n",
       "13696                  Waltrop-Abdinghof  Nordrhein-Westfalen  \n",
       "13700             Gevelsberg-Oberbröking  Nordrhein-Westfalen  \n",
       "13713            Meinerzhagen-Redlendorf  Nordrhein-Westfalen  \n",
       "15000                     Aachen-Orsbach  Nordrhein-Westfalen  \n",
       "\n",
       "[88 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "\n",
    "# isNRW = df_stations['state'] == \"Nordrhein-Westfalen\"\n",
    "isNRW = df_stations['state'].str.contains(\"Nordrhein\")\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "isOperational = (df_stations['date_to'] >= \"2017-12-31\") & (df_stations['date_from'] <= \"2017-01-01\")\n",
    "\n",
    "#isBefore1950 = df_stations['date_from'] < '1950'\n",
    "#dfNRW = df_stations[isNRW & isOperational & isBefore1950]\n",
    "\n",
    "# select on both conditions\n",
    "\n",
    "dfNRW = df_stations[isNRW & isOperational]\n",
    "\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read stations selected in QGIS and exported as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>station_id</th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>216</td>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>1300</td>\n",
       "      <td>2004-06-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>351</td>\n",
       "      <td>51.2540</td>\n",
       "      <td>8.1565</td>\n",
       "      <td>Eslohe</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360</td>\n",
       "      <td>2483</td>\n",
       "      <td>1995-10-12</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>839</td>\n",
       "      <td>51.1803</td>\n",
       "      <td>8.4891</td>\n",
       "      <td>Kahler Asten</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>430</td>\n",
       "      <td>2947</td>\n",
       "      <td>2006-10-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>286</td>\n",
       "      <td>51.1333</td>\n",
       "      <td>8.0348</td>\n",
       "      <td>Lennestadt-Theten</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>477</td>\n",
       "      <td>3215</td>\n",
       "      <td>2007-06-01</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>375</td>\n",
       "      <td>51.1683</td>\n",
       "      <td>8.7125</td>\n",
       "      <td>Medebach-Berge</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fid  station_id   date_from     date_to  altitude  latitude  longitude  \\\n",
       "0   32         216  2004-10-01  2021-01-25       298   51.1143     7.8807   \n",
       "1  213        1300  2004-06-01  2021-01-25       351   51.2540     8.1565   \n",
       "2  360        2483  1995-10-12  2021-01-25       839   51.1803     8.4891   \n",
       "3  430        2947  2006-10-01  2021-01-25       286   51.1333     8.0348   \n",
       "4  477        3215  2007-06-01  2021-01-25       375   51.1683     8.7125   \n",
       "\n",
       "                      name                state  \n",
       "0  Attendorn-Neulisternohl  Nordrhein-Westfalen  \n",
       "1                   Eslohe  Nordrhein-Westfalen  \n",
       "2             Kahler Asten  Nordrhein-Westfalen  \n",
       "3        Lennestadt-Theten  Nordrhein-Westfalen  \n",
       "4           Medebach-Berge  Nordrhein-Westfalen  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_QGIS = pd.read_csv(\"../data/derived/DWD/Maps/DWD_2017_Prec_Stations_Hourly_OE_HSK.csv\")\n",
    "df_QGIS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[216, 1300, 2483, 2947, 3215, 4488, 5468, 6264, 7330]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_ids_selected = list(df_QGIS[\"station_id\"])\n",
    "# station_ids_selected = list(dfNRW.index)\n",
    "station_ids_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server\n",
    "\n",
    "Problem: Not all stations listed in the station description file are associated with a time series (zip file)! The stations in the description file and the set of stations whoch are TS data provided for (zip files) do not match perfectly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stundenwerte_RR_00216_20041001_20191231_hist.zip\n",
      "stundenwerte_RR_01300_20040601_20191231_hist.zip\n",
      "stundenwerte_RR_02483_19951012_20191231_hist.zip\n",
      "stundenwerte_RR_02947_20061001_20191231_hist.zip\n",
      "stundenwerte_RR_03215_20070601_20191231_hist.zip\n",
      "stundenwerte_RR_04488_20060801_20191231_hist.zip\n",
      "stundenwerte_RR_05468_20060701_20191231_hist.zip\n",
      "stundenwerte_RR_06264_20040601_20191231_hist.zip\n",
      "stundenwerte_RR_07330_20051001_20191231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "# Add the names of the zip files only to a list. \n",
    "local_zip_list = []\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join (Merge) the Time Series Columns\n",
    "\n",
    "https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def prec_ts_to_df(fname):\n",
    "    \n",
    "    dateparse = lambda dates: [datetime.strptime(str(d), '%Y%m%d%H') for d in dates]\n",
    "\n",
    "    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "\n",
    "    #df = pd.read_csv(fname, delimiter=\";\", encoding=\"iso8859_2\",\\\n",
    "    #             index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse)\n",
    "    \n",
    "    # https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n",
    "\n",
    "    # Column headers: remove leading blanks (strip), replace \" \" with \"_\", and convert to lower case.\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def temp_ts_to_df(fname):\n",
    "    \n",
    "    dateparse = lambda dates: [datetime.strptime(str(d), '%Y%m%d') for d in dates]\n",
    "\n",
    "    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM_BEGINN\", parse_dates = [\"MESS_DATUM_BEGINN\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "\n",
    "    #df = pd.read_csv(fname, delimiter=\";\", encoding=\"iso8859_2\",\\\n",
    "    #             index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse)\n",
    "    \n",
    "    # https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n",
    "\n",
    "    # Column headers: remove leading blanks (strip), replace \" \" with \"_\", and convert to lower case.\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRECIPITATION\n",
    "def prec_ts_merge():\n",
    "    # Very compact code.\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = prec_ts_to_df(myfile)\n",
    "                s = dftmp[\"r1\"].rename(dftmp[\"stations_id\"][0]).to_frame()\n",
    "                # outer merge.\n",
    "                df = pd.merge(df, s, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    #df.index.names = [\"year\"]\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPERATURE\n",
    "def temp_ts_merge():\n",
    "    # Very compact code.\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = temp_ts_to_df(myfile)\n",
    "                s = dftmp[\"ja_tt\"].rename(dftmp[\"stations_id\"][0]).to_frame()\n",
    "                # outer merge.\n",
    "                df = pd.merge(df, s, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    #df.index.names = [\"year\"]\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00216_20041001_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041001_20191231_00216.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01300_20040601_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040601_20191231_01300.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02483_19951012_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19951012_20191231_02483.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02947_20061001_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20061001_20191231_02947.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03215_20070601_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20070601_20191231_03215.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04488_20060801_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060801_20191231_04488.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05468_20060701_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060701_20191231_05468.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_06264_20040601_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040601_20191231_06264.txt\n",
      "\n",
      "Zip archive: ../data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_07330_20051001_20191231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20051001_20191231_07330.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merged_ts = prec_ts_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211398, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>216</th>\n",
       "      <th>1300</th>\n",
       "      <th>2483</th>\n",
       "      <th>2947</th>\n",
       "      <th>3215</th>\n",
       "      <th>4488</th>\n",
       "      <th>5468</th>\n",
       "      <th>6264</th>\n",
       "      <th>7330</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-31 19:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 20:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 22:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     216   1300  2483  2947  3215  4488  5468  6264  7330\n",
       "time                                                                     \n",
       "2019-12-31 19:00:00   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "2019-12-31 20:00:00   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "2019-12-31 21:00:00   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "2019-12-31 22:00:00   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
       "2019-12-31 23:00:00   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_ts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prec or temp?\n",
    "filepathname = local_ts_merged_dir + \"prec_ts_merged.csv\"\n",
    "print(\"df_merged_ts is saved to: %s\" % (filepathname))\n",
    "df_merged_ts.to_csv(filepathname,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi= 136, figsize=(8,4))\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax4 = fig.add_subplot(224)\n",
    "df_merged_ts[1303].plot(ax = ax1)\n",
    "df_merged_ts[2110].plot(ax = ax1)\n",
    "df_merged_ts[2110].plot(ax = ax2)\n",
    "df_merged_ts[1590].plot(ax = ax3)\n",
    "df_merged_ts[2110].plot(ax = ax4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention: The following code creates a heatmap. This makes only sense if the data density is low, e.g. annual temperatures. Do not use it for hourly precipitation over a long period.**\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot\n",
    "sns.set_style('ticks')\n",
    "fig1, ax1 = plt.subplots(dpi = 400, figsize = (12,24))\n",
    "\n",
    "#sns.heatmap(df_merged_ts, cmap='RdYlGn_r', annot=False, ax = ax1)\n",
    "sns.heatmap(df_merged_ts, cmap='coolwarm', annot=True, vmin = 8, vmax = 12, ax = ax1)\n",
    "\n",
    "# _r reverses the normal order of the color map 'RdYlGn'\n",
    "\n",
    "#sns.heatmap(df, cmap='coolwarm', annot=True, vmin = 8, vmax = 12, ax = ax)\n",
    "ax1.set_yticklabels(df_merged_ts.index.strftime('%Y'))\n",
    "plt.show()\n",
    "fig1.savefig('example1.png')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed = df_merged_ts.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed.index.names = ['station_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathname = local_ts_merged_dir + \"prec_ts_merged_transposed.csv\"\n",
    "print(\"df_merged_ts_transposed is saved to: %s\" % (filepathname))\n",
    "df_merged_ts_transposed.to_csv(filepathname,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed.iloc[2,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed_last_day = df_merged_ts_transposed.iloc[:,-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed_last_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed_last_day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathname = local_ts_merged_dir + \"prec_ts_merged_transposed_last_day.csv\"\n",
    "print(\"df_merged_ts_transposed_last_day saved to: %s\" % (filepathname))\n",
    "df_merged_ts_transposed_last_day.to_csv(filepathname,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_append():\n",
    "    # Very compact code.\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "# TEMPERATURE                dftmp = temp_ts_to_df(myfile)\n",
    "# PRECIPIATION\n",
    "                dftmp = prec_ts_to_df(myfile)\n",
    "                dftmp = dftmp.merge(df_stations,how=\"inner\",left_on=\"stations_id\",right_on=\"station_id\",right_index=True)\n",
    "#                print(dftmp.head(5))\n",
    "                df = df.append(dftmp)\n",
    "\n",
    "    #df.index.names = [\"year\"]\n",
    "    #df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts = ts_append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathname = local_ts_appended_dir + \"prec_ts_appended.csv\"\n",
    "print(\"df_appended_ts saved to: %s\" % (filepathname))\n",
    "df_appended_ts.to_csv(filepathname,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_appended_ts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_appended_ts.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime.fromisoformat(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts[[\"r1\",\"longitude\", \"latitude\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = datetime.fromisoformat('2019-07-08 02:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df_appended_ts.index >= '2020-06-01 00:00:00') & (df_appended_ts.index < '2020-07-01 00:00:00') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts_june = df_appended_ts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathname = local_ts_appended_dir + \"prec_ts_appended_june.csv\"\n",
    "print(\"df_appended_ts_june saved to: %s\" % (filepathname))\n",
    "df_appended_ts_june.to_csv(filepathname,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts_june[df_appended_ts_june[\"r1\"] >= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
